{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# go wide screen\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version implements a \n",
    "# spectrally variing detection threshold.\n",
    "import time\n",
    "import numpy as np\n",
    "import spectrum\n",
    "from collections import OrderedDict\n",
    "import argparse\n",
    "from astropy.io import fits\n",
    "from astropy import wcs\n",
    "import sys\n",
    "from astropy.io import ascii\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "#from imageio import imread\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import maxflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp(s):\n",
    "    print(s)\n",
    "    return s + \"\\n\"\n",
    "\n",
    "\n",
    "def grow_segment(c, outmap, x, y, z, threshold, label):\n",
    "    # Non object oriented version\n",
    "    def get_children(zyx, shape):\n",
    "        maxz, maxy, maxx = shape\n",
    "        z, y, x = zyx\n",
    "        children = []\n",
    "        ddx = [-1,0,1]\n",
    "        ddy = [-1,0,1]\n",
    "        ddz = [-1,0,1]\n",
    "        #ddz = [0]\n",
    "        for dx in ddx:\n",
    "            for dy in ddy:\n",
    "                for dz in ddz:\n",
    "                    if not dx == dy == dz == 0:\n",
    "                        newx, newy, newz = x+dx, y+dy, z+dz\n",
    "                        if newx >= 0 and newx < maxx\\\n",
    "                            and newy >= 0 and newy < maxy\\\n",
    "                            and newz >= 0 and newz < maxz:\n",
    "                               children.append( (newz, newy, newx) ) \n",
    "        return children\n",
    "\n",
    "    pixel_list = []\n",
    "    stack = [(z,y,x)]\n",
    "    outmap[(z,y,x)] = label\n",
    "\n",
    "    maxiter = 1e6\n",
    "    iter = 0\n",
    "    while len(stack) > 0: \n",
    "        pm = stack.pop()\n",
    "        pp = get_children(pm, c.shape)\n",
    "        for p in pp:\n",
    "            if c[p] >= threshold and  outmap[p] == 0: # pixel that are labeled 0 have not been visited yet\n",
    "                stack.append(p)\n",
    "                outmap[p] = label\n",
    "            elif c[p] < threshold and  outmap[p] == 0:\n",
    "                outmap[p] = -1 # label pixel with -1 if they have beed visited already\n",
    "        iter += 1\n",
    "        if iter > maxiter:\n",
    "            break\n",
    "\n",
    "    return outmap#pixel_list            \n",
    "\n",
    "\n",
    "def build_map(ww, c, sigma_detect_threshold, sigma_grow_threshold, noise_model):\n",
    "    # run cloud growth algorithm on all pixels above detection threshold\n",
    "    # try to be intelligent, only loop over pixels that exceed detection threshold\n",
    "    ii = (c.swapaxes(2,0) > np.polyval(noise_model, ww) * sigma_detect_threshold ).swapaxes(2,0)\n",
    "    \n",
    "    \n",
    "    N =  (np.sum(ii))\n",
    "    print(\"{} pixel above detection threshold\".format(N))\n",
    "\n",
    "    outmap = np.zeros_like(c, dtype=int)\n",
    "\n",
    "    zz,yy,xx = [np.arange(s, dtype=int) for s in c.shape]\n",
    "    YY,ZZ,XX = np.meshgrid(yy, zz, xx)\n",
    "\n",
    "    label = 1\n",
    "    for i, (x,y,z) in enumerate( zip(XX[ii], YY[ii], ZZ[ii] ) )  :  \n",
    "        if outmap[z,y,x] == 0:\n",
    "            pp(\"### {} z={} ###\".format(i,z))\n",
    "            print(x,y,z, outmap[z,y,x])\n",
    "            #outmap = build_map2(c, outmap, x,y,z, threshold = grow_threshold, label=label)\n",
    "\n",
    "            summary = \"\"\n",
    "            summary += pp(\"Building map starting with pixel {} out of {} that exceeds threshold...\".format(i,N))\n",
    "            start_time = time.time()\n",
    "            #print(\"{} labeled pixels on map\".format( np.nansum( (outmap > 0).flatten())) )\n",
    "            \n",
    "            # Here we set to which threshold a region shoudl be grown.\n",
    "            # note that this is not stricly correct, the threshold should very also inside\n",
    "            # grow_segment as function of wavelength. Here we will just rely on this\n",
    "            # being a slowly variing function.\n",
    "            threshold = np.polyval(noise_model, ww[z]) * sigma_grow_threshold\n",
    "            print(\"grow_threshold = \", threshold)\n",
    "            outmap = grow_segment(c, outmap, x,y,z, threshold, label=label)\n",
    "            time_to_build = time.time() - start_time\n",
    "            summary += pp(\"Time to build map: {:.4e} s\".format(time_to_build))\n",
    "\n",
    "            print(\"{} labeled pixels on map\".format( np.nansum( (outmap > 0).flatten())) )\n",
    "            print(\"{} untouched pixels on map\".format( np.nansum( (outmap == 0).flatten())) )\n",
    "\n",
    "        label += 1\n",
    "\n",
    "        if i > 1e6:\n",
    "            break\n",
    "    #print (cnt)\n",
    "    return outmap\n",
    "\n",
    "\n",
    "def filter_minsize(outmap, minsize):\n",
    "    # filter regions with too small volume \n",
    "    rr = np.sort( np.unique( outmap.flatten() ) )\n",
    "    for r in rr:\n",
    "        if not r > 0:\n",
    "            continue\n",
    "        N = np.sum( outmap == r )\n",
    "        #print(\"{} : N = {}\".format(r, N))\n",
    "        if N < minsize:\n",
    "            outmap[outmap == r] = -1  \n",
    "    rr = np.sort( np.unique( outmap.flatten() ) )\n",
    "    \n",
    "    # relabel\n",
    "    for i,r in enumerate(rr[rr>0]):\n",
    "        outmap[outmap == r] = i + 1\n",
    "\n",
    "    rr = np.sort( np.unique( outmap.flatten() ) )\n",
    "    print(\"{} regions survive size cut\".format( len(rr[rr>0]) ))\n",
    "\n",
    "    return outmap\n",
    "\n",
    "\n",
    "\n",
    "def save_map(outmap, fmapout):\n",
    "    w = wcs.WCS(s.hdu)\n",
    "    # save map\n",
    "    f = np.zeros_like(c)\n",
    "    f[outmap > 0] = c[outmap > 0]\n",
    "    wcs_header =  w.to_header()\n",
    "\n",
    "\n",
    "    h = fits.PrimaryHDU(data=outmap, header=s.hdu.header)\n",
    "    for k in wcs_header:\n",
    "        h.header[k] = wcs_header[k]\n",
    "    hdu = fits.HDUList(h)\n",
    "\n",
    "    # save map filtered data\n",
    "    f = np.zeros_like(c)\n",
    "    f[outmap > 0] = c[outmap > 0]\n",
    "    h = fits.ImageHDU(data=f, header=s.hdu.header, name = \"filtered_data\")\n",
    "    for k in wcs_header:\n",
    "        h.header[k] = wcs_header[k]\n",
    "\n",
    "    hdu.append(h)\n",
    "\n",
    "    # save shells \n",
    "    f = np.zeros_like(c)\n",
    "    f[outmap == -1] = c[outmap == -1]\n",
    "\n",
    "    h = fits.ImageHDU(data=f, header=s.hdu.header, name = \"shells\")\n",
    "    for k in wcs_header:\n",
    "        h.header[k] = wcs_header[k]\n",
    "    hdu.append(h)\n",
    "\n",
    "    hdu.writeto(fmapout, overwrite=True)\n",
    "    print(\"Wrote {}.\".format(fmapout))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fast graph construction, no color or distance based weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcube = \"sf2outcube_COSMOSA_022_pca.fits.gz\"\n",
    "fnoisemodel = \"sf2outcube_COSMOSC_022_pca.detect_noise_model\"\n",
    "workdir = \"/work/04287/mxhf/maverick/hetdex/cubes\"\n",
    "datadir = os.path.join(workdir, \"data\")\n",
    "\n",
    "w = 12.\n",
    "wz = .05\n",
    "\n",
    "dscale = 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "s = spectrum.readSpectrum(os.path.join(datadir, fcube))\n",
    "ww = s.grid()\n",
    "\n",
    "# read noise model\n",
    "noise_model = np.loadtxt(os.path.join(datadir, fnoisemodel) )\n",
    "\n",
    "# scale date by noise\n",
    "c = s.data\n",
    "\n",
    "c = dscale * np.multiply(c.T, 1./np.polyval(noise_model, ww )).T\n",
    "\n",
    "#c = c[118-5:118+6] # for faster testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph with integer capacities.\n",
    "# best guess for number of nodes and edges\n",
    "nnodes = len( c.flatten() )\n",
    "nedges = len( c.flatten() )*26 + 2*len( c.flatten() )\n",
    "g = maxflow.Graph[float](nnodes, nedges )\n",
    "\n",
    "nodes = g.add_grid_nodes(c.shape)\n",
    "\n",
    "# connection weights based on Euclidian distances\n",
    "structure = np.array([ w*wz* np.array([[.58, .7, .58],\n",
    "                        [.7, 1, .7],\n",
    "                        [.58, .7, .58]]),\n",
    "                       w*np.array([[.7, 1, .7],\n",
    "                        [1, 0, 1],\n",
    "                        [.7, 1, .7]]),\n",
    "                       w*wz*np.array([[.58, .7, .58],\n",
    "                        [.7, 1, .7],\n",
    "                        [.58, .7, .58]]) ])\n",
    "\n",
    "g.add_grid_edges(nodes, structure=structure)\n",
    "g.add_grid_tedges(nodes, c, 1.-c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum flow.\n",
    "g.maxflow()\n",
    "\n",
    "# Get the segments.\n",
    "sgm = g.get_grid_segments(nodes)\n",
    "\n",
    "# The labels should be 1 where sgm is False and 0 otherwise.\n",
    "img2 = np.int_(np.logical_not(sgm))\n",
    "# Show the result.\n",
    "cout = img2.reshape(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subcube = c[118-2:118+3]\n",
    "#subcout = cout[118-2:118+3]\n",
    "subcube = c\n",
    "subcout = cout\n",
    "N = subcube.shape[0]\n",
    "if N < 15:\n",
    "    f = plt.figure(figsize=[15,10])\n",
    "    for i in range(N):\n",
    "        plt.subplot(1,N,i+1)\n",
    "        plt.imshow(subcube[i]/dscale, origin='bottom', interpolation='none',vmin=0., vmax=2.)\n",
    "\n",
    "    f = plt.figure(figsize=[15,10])\n",
    "    for i in range(N):\n",
    "        plt.subplot(1,N,i+1)\n",
    "        plt.imshow(subcout[i], origin='bottom', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = s.hdu\n",
    "h.data = cout\n",
    "h.writeto(os.path.join(datadir, \"ct\" + fcube), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# production code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input datacube /work/04287/mxhf/maverick/hetdex/cubes/data/sf2outcube_COSMOSA_022_pca.fits.gz.\n"
     ]
    }
   ],
   "source": [
    "# This version implements a \n",
    "# spectrally variing detection threshold.\n",
    "import time\n",
    "import numpy as np\n",
    "import spectrum\n",
    "from collections import OrderedDict\n",
    "import argparse\n",
    "from astropy.io import fits\n",
    "from astropy import wcs\n",
    "import sys\n",
    "from astropy.io import ascii\n",
    "import os\n",
    "\n",
    "\n",
    "def pp(s):\n",
    "    print(s)\n",
    "    return s + \"\\n\"\n",
    "\n",
    "\n",
    "def grow_segment(c, outmap, x, y, z, threshold, label):\n",
    "    # Non object oriented version\n",
    "    def get_children(zyx, shape):\n",
    "        maxz, maxy, maxx = shape\n",
    "        z, y, x = zyx\n",
    "        children = []\n",
    "        ddx = [-1,0,1]\n",
    "        ddy = [-1,0,1]\n",
    "        ddz = [-1,0,1]\n",
    "        #ddz = [0]\n",
    "        for dx in ddx:\n",
    "            for dy in ddy:\n",
    "                for dz in ddz:\n",
    "                    if not dx == dy == dz == 0:\n",
    "                        newx, newy, newz = x+dx, y+dy, z+dz\n",
    "                        if newx >= 0 and newx < maxx\\\n",
    "                            and newy >= 0 and newy < maxy\\\n",
    "                            and newz >= 0 and newz < maxz:\n",
    "                               children.append( (newz, newy, newx) ) \n",
    "        return children\n",
    "\n",
    "    pixel_list = []\n",
    "    stack = [(z,y,x)]\n",
    "    outmap[(z,y,x)] = label\n",
    "\n",
    "    maxiter = 1e6\n",
    "    iter = 0\n",
    "    while len(stack) > 0: \n",
    "        pm = stack.pop()\n",
    "        pp = get_children(pm, c.shape)\n",
    "        for p in pp:\n",
    "            if c[p] >= threshold and  outmap[p] == 0: # pixel that are labeled 0 have not been visited yet\n",
    "                stack.append(p)\n",
    "                outmap[p] = label\n",
    "            elif c[p] < threshold and  outmap[p] == 0:\n",
    "                outmap[p] = -1 # label pixel with -1 if they have beed visited already\n",
    "        iter += 1\n",
    "        if iter > maxiter:\n",
    "            break\n",
    "\n",
    "    return outmap#pixel_list            \n",
    "\n",
    "\n",
    "\n",
    "def build_map(detectseeds, ww, c, sigma_detect_threshold, sigma_grow_threshold, noise_model):\n",
    "    # run cloud growth algorithm on all pixels seeded pixels\n",
    "    # try to be intelligent, only loop over pixels that exceed detection threshold\n",
    "\n",
    "\n",
    "    ii = detectseeds\n",
    "    \n",
    "    outmap = np.zeros_like(c, dtype=int)\n",
    "\n",
    "    zz,yy,xx = [np.arange(s, dtype=int) for s in c.shape]\n",
    "    YY,ZZ,XX = np.meshgrid(yy, zz, xx)\n",
    "\n",
    "    label = 1\n",
    "    for i, (x,y,z) in enumerate( zip(XX[ii], YY[ii], ZZ[ii] ) )  :  \n",
    "        if outmap[z,y,x] == 0:\n",
    "            pp(\"### {} z={} ###\".format(i,z))\n",
    "            print(x,y,z, outmap[z,y,x])\n",
    "            #outmap = build_map2(c, outmap, x,y,z, threshold = grow_threshold, label=label)\n",
    "\n",
    "            summary = \"\"\n",
    "            summary += pp(\"Building map starting with pixel {} out of {} that exceeds threshold...\".format(i,N))\n",
    "            start_time = time.time()\n",
    "            #print(\"{} labeled pixels on map\".format( np.nansum( (outmap > 0).flatten())) )\n",
    "            \n",
    "            # Here we set to which threshold a region shoudl be grown.\n",
    "            # note that this is not stricly correct, the threshold should very also inside\n",
    "            # grow_segment as function of wavelength. Here we will just rely on this\n",
    "            # being a slowly variing function.\n",
    "            threshold = np.polyval(noise_model, ww[z]) * sigma_grow_threshold\n",
    "            print(\"grow_threshold = \", threshold)\n",
    "            outmap = grow_segment(c, outmap, x,y,z, threshold, label=label)\n",
    "            time_to_build = time.time() - start_time\n",
    "            summary += pp(\"Time to build map: {:.4e} s\".format(time_to_build))\n",
    "\n",
    "            print(\"{} labeled pixels on map\".format( np.nansum( (outmap > 0).flatten())) )\n",
    "            print(\"{} untouched pixels on map\".format( np.nansum( (outmap == 0).flatten())) )\n",
    "\n",
    "        label += 1\n",
    "\n",
    "        if i > 1e6:\n",
    "            break\n",
    "    #print (cnt)\n",
    "    return outmap\n",
    "\n",
    "\n",
    "def filter_minsize(outmap, minsize):\n",
    "    # filter regions with too small volume \n",
    "    rr = np.sort( np.unique( outmap.flatten() ) )\n",
    "    for r in rr:\n",
    "        if not r > 0:\n",
    "            continue\n",
    "        N = np.sum( outmap == r )\n",
    "        #print(\"{} : N = {}\".format(r, N))\n",
    "        if N < minsize:\n",
    "            outmap[outmap == r] = -1  \n",
    "    rr = np.sort( np.unique( outmap.flatten() ) )\n",
    "    \n",
    "    # relabel\n",
    "    for i,r in enumerate(rr[rr>0]):\n",
    "        outmap[outmap == r] = i + 1\n",
    "\n",
    "    rr = np.sort( np.unique( outmap.flatten() ) )\n",
    "    print(\"{} regions survive size cut\".format( len(rr[rr>0]) ))\n",
    "\n",
    "    return outmap\n",
    "\n",
    "\n",
    "\n",
    "def save_map(outmap, fmapout):\n",
    "    w = wcs.WCS(s.hdu)\n",
    "    # save map\n",
    "    f = np.zeros_like(c)\n",
    "    f[outmap > 0] = c[outmap > 0]\n",
    "    wcs_header =  w.to_header()\n",
    "\n",
    "\n",
    "    h = fits.PrimaryHDU(data=outmap, header=s.hdu.header)\n",
    "    for k in wcs_header:\n",
    "        h.header[k] = wcs_header[k]\n",
    "    hdu = fits.HDUList(h)\n",
    "\n",
    "    # save map filtered data\n",
    "    f = np.zeros_like(c)\n",
    "    f[outmap > 0] = c[outmap > 0]\n",
    "    h = fits.ImageHDU(data=f, header=s.hdu.header, name = \"filtered_data\")\n",
    "    for k in wcs_header:\n",
    "        h.header[k] = wcs_header[k]\n",
    "\n",
    "    hdu.append(h)\n",
    "\n",
    "    # save shells \n",
    "    f = np.zeros_like(c)\n",
    "    f[outmap == -1] = c[outmap == -1]\n",
    "\n",
    "    h = fits.ImageHDU(data=f, header=s.hdu.header, name = \"shells\")\n",
    "    for k in wcs_header:\n",
    "        h.header[k] = wcs_header[k]\n",
    "    hdu.append(h)\n",
    "\n",
    "    hdu.writeto(fmapout, overwrite=True)\n",
    "    print(\"Wrote {}.\".format(fmapout))\n",
    "\n",
    "    \n",
    "    \n",
    "COMMANDLINE = False\n",
    "\n",
    "if COMMANDLINE:\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-m\", \"--minsize\", type=int, default=3,\n",
    "                        help=\"Minimum region size.\")\n",
    "    parser.add_argument(\"-g\", \"--sigma_grow_threshold\", type=float,  default=3.,\n",
    "                        help=\"Region growth threshold.\")\n",
    "    parser.add_argument(\"-d\",\"--sigma_detect_threshold\", type=float, default=5.,\n",
    "                        help=\"Detection threshold.\")\n",
    "    parser.add_argument('-i', '--infile', type=str,                    \n",
    "                        help='Input cube.')\n",
    "    parser.add_argument('-o', '--outfile', type=str, default='',\n",
    "                        help='Output map.')\n",
    "    parser.add_argument('-b', '--bad_regions', type=str, default='',\n",
    "                        help='Output map.')\n",
    "\n",
    "    parser.add_argument('-n', '--noisemodel', type=str, default='',\n",
    "                        help='File containing the polynomial parameters for the noise model.')\n",
    "\n",
    "    args = parser.parse_args(sys.argv[1:])\n",
    "\n",
    "\n",
    "    fcube = args.infile\n",
    "    fmapout = args.outfile\n",
    "    sigma_detect_threshold = args.sigma_detect_threshold\n",
    "    sigma_grow_threshold = args.sigma_grow_threshold\n",
    "    minsize = args.minsize\n",
    "    noise_model = np.loadtxt(args.noisemodel)\n",
    "    bad_regions = args.bad_regions\n",
    "    \n",
    "else:\n",
    "    \n",
    "    workdir = \"/work/04287/mxhf/maverick/hetdex/cubes/data\"\n",
    "\n",
    "    fcube = workdir + \"/sf2outcube_COSMOSA_022_pca.fits.gz\"\n",
    "    fnoisemodel = workdir + \"/sf2outcube_COSMOSC_022_pca.detect_noise_model\"\n",
    "\n",
    "    fmapout = workdir + \"/map_COSMOSA_022_pca.fits.gz\"\n",
    "    bad_regions = \"\"\n",
    "    sigma_detect_threshold = 5.\n",
    "    sigma_grow_threshold = 3.\n",
    "    minsize = 3.\n",
    "    noise_model = np.loadtxt(fnoisemodel)\n",
    "\n",
    "\n",
    "    w = 12.\n",
    "    wz = .05\n",
    "    dscale = 5.\n",
    "\n",
    "\n",
    "    \n",
    "badwlregions = []\n",
    "\n",
    "if bad_regions != '':\n",
    "    if not os.path.exists(args.bad_regions):\n",
    "        print(\"WARNING: {} not found proceeding without.\".format(args.bad_regions))\n",
    "    else:\n",
    "        t = ascii.read(args.bad_regions)\n",
    "        for r in t:\n",
    "            badwlregions.append([r[\"start\"],r[\"end\"]])\n",
    "\n",
    "print(\"Reading input datacube {}.\".format(fcube))   \n",
    "s = spectrum.readSpectrum(fcube)\n",
    "ww = s.grid()\n",
    "c = s.data\n",
    "for r in badwlregions:  \n",
    "    c[r[0]:r[1]] = 0. # take out bad region\n",
    "    \n",
    "# The detects seeds array is a boolean array of the same dimensions\n",
    "# as the input datacube.\n",
    "# It flags pixels as potential objects.\n",
    "# Here we siple falg all pixel that have a value of sigma_detect_threshold x noise.\n",
    "detectseeds = (c.swapaxes(2,0) > np.polyval(noise_model, ww) * sigma_detect_threshold ).swapaxes(2,0)\n",
    "\n",
    "\n",
    "N =  (np.sum(detectseeds))\n",
    "print(\"{} pixel are flages as detection seeds\".format(N))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "outmap = build_map(detectseeds, ww, c, sigma_detect_threshold, sigma_grow_threshold, noise_model)\n",
    "outmap = filter_minsize(outmap, minsize)\n",
    "\n",
    "\n",
    "save_map(outmap, fmapout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
